{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Model: NLP + ANN\n",
    "#####################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, LSTM,Input, Dropout, Embedding\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "SEED = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "callback = EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "p_data_raw       = os.path.abspath('../data/raw') # raw data path\n",
    "p_data_processed = os.path.abspath('../data/processed') # processed data path\n",
    "p_data_cleaned   = os.path.abspath('../data/cleaned') # clearned data path\n",
    "p_data_src       = os.path.abspath('../data/src') # src data path\n",
    "\n",
    "k_ratings_f = pd.read_csv(os.path.abspath(p_data_processed+'/k_ratings_f.csv'), dtype={'label_c': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get rid of \"Other\" genre\n",
    "# k_ratings_f = (k_ratings_f[k_ratings_f['genre1']!='Other'])\n",
    "# vote_count=k_ratings_f.groupby('movieId')['userId'].count().sort_values(ascending=False)\n",
    "# # get rid of movies with <10 ratings\n",
    "# k_ratings_f = k_ratings_f[k_ratings_f.movieId.isin(vote_count[vote_count>=10].index.to_list())]  # 245 movies\n",
    "# save data\n",
    "# k_ratings_f.to_csv(os.path.abspath(os.path.abspath(p_data_processed+'/k_ratings_f.csv')),index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MovieLens datasets\n",
    "# ml_links   = pd.read_csv(p_data_raw+'/ml-25m/links.csv') # movie links\n",
    "# ml_ratings = pd.read_csv(p_data_raw+'/ml-25m/ratings.csv') # user ratings\n",
    "# ml_movies  = pd.read_csv(p_data_raw+'/ml-25m/movies.csv') # movie list\n",
    "\n",
    "# # IMDB Kmovie list\n",
    "# imdb_mat_orig = pd.read_pickle(p_data_raw+'/imdb_kr.pickle')\n",
    "\n",
    "# # IMDB Kmovie list join MovieLens\n",
    "# imdb_mat = pd.merge(imdb_mat_orig,ml_links,on='imdbId')\n",
    "\n",
    "# # IMDB Kmovie list join MovieLens movie list\n",
    "# imdb_mat = pd.merge(imdb_mat,ml_movies,on='movieId')\n",
    "\n",
    "# imdb_mat_f = imdb_mat[imdb_mat.imdb_id.isin(k_ratings_f.imdb_id.to_list())]\n",
    "# imdb_mat_f['genre1'] = imdb_mat_f['genres'].apply(lambda s:s.split('|')[0])\n",
    "# imdb_mat_f['year'] = imdb_mat_f['title'].apply(lambda s:s.split()[-1].replace('(','').replace(')',''))\n",
    "# imdb_mat_f.to_csv(os.path.abspath(p_data_processed+'/imdb_mat_f.csv'),index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare X\n",
    "X = pd.concat([k_ratings_f.drop(['userId','movieId','rating_x','timestamp','name','year','rating_y',\n",
    "                                 # 'metascore',\n",
    "                                 'imdb_id','poster','genre',\n",
    "                                 'certificate','runtime','director_actor', \n",
    "                                 # 'story', \n",
    "                                 'imdbId','tmdbId',\n",
    "                                 'title','genres','genre1',\n",
    "                                 'label','label_c',\n",
    "                                 'y_5','y',\n",
    "                                 # 'timestamp','vote',\n",
    "                                 'director','director1','director2','actor1'\n",
    "                                 ], axis = 1),\n",
    "               pd.get_dummies(k_ratings_f[[\n",
    "                   #'certificate',\n",
    "                   #'genre1',\n",
    "                   'label_c','director1','actor1']], \n",
    "                              drop_first = True, dummy_na = True)],axis = 1)\n",
    "\n",
    "# fill metascore null values with mean\n",
    "X['metascore'].fillna(X['metascore'].mean(), inplace=True)\n",
    "X.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare y\n",
    "# y = k_ratings_f['y']\n",
    "y = k_ratings_f['y_5']\n",
    "# y = k_ratings_f['y'].cat.codes\n",
    "\n",
    "# label_encoder object knows how to understand word labels.\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Encode target labels\n",
    "y = label_encoder.fit_transform(y)\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare X1 for NLP\n",
    "def preprocess_text(sen):\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)            \n",
    "    # Removing \"See full summary Â»\"\n",
    "    sentence = sentence.replace('See full summary', ' ')\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = []\n",
    "sentences = list(X[\"story\"])\n",
    "for sen in sentences:\n",
    "    X1.append(preprocess_text(sen))\n",
    "    \n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X1)\n",
    "\n",
    "X1 = tokenizer.texts_to_sequences(X1)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 50 # 200\n",
    "\n",
    "X1 = pad_sequences(X1, padding='post', maxlen=maxlen)\n",
    "\n",
    "# data scale\n",
    "# scaler1 = MinMaxScaler()\n",
    "# scaler1 = StandardScaler()\n",
    "# X1 = scaler1.fit_transform(X1)\n",
    "\n",
    "# embedding\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file_link = p_data_src+'/glove.6B/glove.6B.50d.txt' # glove file link\n",
    "\n",
    "with open(glove_file_link, 'r') as glove_file:\n",
    "    for line in glove_file:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary [word] = vector_dimensions\n",
    "\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare X2 for ANN\n",
    "X2 = X.drop('story', axis=1).copy().values\n",
    "\n",
    "# data scale\n",
    "# scaler2 = MinMaxScaler()\n",
    "scaler2 = StandardScaler()\n",
    "X2 = scaler2.fit_transform(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 385)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 972)          375192      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1944)         1891512     dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 972)          1890540     dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 50)       100000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 486)          472878      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 128)          91648       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 243)          118341      dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 371)          0           lstm[0][0]                       \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           3720        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 5)            55          dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,943,886\n",
      "Trainable params: 4,843,886\n",
      "Non-trainable params: 100,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# model\n",
    "################\n",
    "\n",
    "input_1 = Input(shape=(maxlen,))\n",
    "input_2 = Input(shape=(X.shape[1]-1,))\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, 50, weights=[embedding_matrix], trainable=False)(input_1)\n",
    "LSTM_Layer_1 = LSTM(128)(embedding_layer)\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    dense_layer_1 = Dense(972, activation='relu')(input_2)\n",
    "    dense_layer_2 = Dense(1944, activation='relu')(dense_layer_1)\n",
    "    dense_layer_3 = Dense(972, activation='relu')(dense_layer_2)\n",
    "    dense_layer_4 = Dense(486, activation='relu')(dense_layer_3)\n",
    "    dense_layer_5 = Dense(243, activation='relu')(dense_layer_4)    \n",
    "    \n",
    "    concat_layer = Concatenate()([LSTM_Layer_1, dense_layer_5])\n",
    "    dense_layer_6 = Dense(10, activation='relu')(concat_layer)\n",
    "    output = Dense(5, activation='softmax')(dense_layer_6)\n",
    "    # output = Dense(3, activation='softmax')(dense_layer_6)\n",
    "    \n",
    "    model = Model(inputs=[input_1, input_2], outputs=output)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    #plot_model(model, to_file='figures/model_plot_final.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "model=create_model()\n",
    "# history = model.fit(x=[X1_train, X2_train], y=y_train, validation_data=([X1_test, X2_test],y_test),\n",
    "#                     callbacks=[callback],\n",
    "#                     batch_size=128, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31378 samples\n",
      "Epoch 1/10\n",
      "31378/31378 [==============================] - 58s 2ms/sample - loss: 1.2144 - accuracy: 0.4809\n",
      "Epoch 2/10\n",
      "31378/31378 [==============================] - 57s 2ms/sample - loss: 1.1418 - accuracy: 0.5161\n",
      "Epoch 3/10\n",
      "31378/31378 [==============================] - 58s 2ms/sample - loss: 1.1076 - accuracy: 0.5311\n",
      "Epoch 4/10\n",
      "31378/31378 [==============================] - 60s 2ms/sample - loss: 1.0951 - accuracy: 0.5344\n",
      "Epoch 5/10\n",
      "31378/31378 [==============================] - 59s 2ms/sample - loss: 1.0881 - accuracy: 0.5385\n",
      "Epoch 6/10\n",
      "31378/31378 [==============================] - 60s 2ms/sample - loss: 1.0786 - accuracy: 0.5423\n",
      "Epoch 7/10\n",
      "31378/31378 [==============================] - 56s 2ms/sample - loss: 1.0685 - accuracy: 0.5442\n",
      "Epoch 8/10\n",
      "31378/31378 [==============================] - 59s 2ms/sample - loss: 1.0617 - accuracy: 0.5454\n",
      "Epoch 9/10\n",
      "31378/31378 [==============================] - 60s 2ms/sample - loss: 1.0524 - accuracy: 0.5517\n",
      "Epoch 10/10\n",
      "31378/31378 [==============================] - 61s 2ms/sample - loss: 1.0447 - accuracy: 0.5503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x146e072b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[X1, X2], y=y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler2_final.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model.save(\"model_final.h5\")\n",
    "#joblib.dump(scaler1,'scaler1_final.pkl')\n",
    "joblib.dump(scaler2,'scaler2_final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "ratings_model = load_model('model_final.h5')\n",
    "#ratings_scaler1 = joblib.load('scaler1_final.pkl')\n",
    "ratings_scaler2 = joblib.load('scaler2_final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# reviewers: 9765\n",
      "# movies: 245\n",
      "# movies x reviewers: 2392425\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# prepare X for megamat\n",
    "#####################################\n",
    "reviewers_mat = k_ratings_f[['userId','label','label_c']].copy()\n",
    "movies_mat = k_ratings_f.drop(['userId','rating_x','timestamp','label','label_c','y','y_5'], axis=1).copy()\n",
    "# movies_mat = k_ratings_f.drop(['userId','rating_x','timestamp','label','label_c','story'], axis=1).copy()\n",
    "# storys_mat = k_ratings_f[['movieId','story']].copy()\n",
    "\n",
    "reviewers_mat = reviewers_mat.drop_duplicates(subset='userId')\n",
    "movies_mat = movies_mat.drop_duplicates(subset='movieId')\n",
    "# storys_mat = storys_mat.drop_duplicates(subset='movieId')\n",
    "\n",
    "print('# reviewers: {}'.format(len(reviewers_mat)))\n",
    "print('# movies: {}'.format(len(movies_mat)))\n",
    "\n",
    "# create megamat\n",
    "reviewers_mat['joincol'] = 1\n",
    "movies_mat['joincol'] = 1\n",
    "megamat = pd.merge(reviewers_mat,movies_mat,how='outer',on='joincol')\n",
    "print('# movies x reviewers: {}'.format(len(megamat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# movies x reviewers exclude real reviews: 2361047\n"
     ]
    }
   ],
   "source": [
    "# exclude ratings with real \"y\"\n",
    "k_ratings_f['joinId'] = k_ratings_f['userId'].astype(str) + '_' + k_ratings_f['movieId'].astype(str)\n",
    "# take long time!\n",
    "megamat['joinId'] = megamat['userId'].astype(str) + '_' + megamat['movieId'].astype(str)\n",
    "megamat = megamat[~megamat.joinId.isin(k_ratings_f['joinId'].tolist())]\n",
    "print('# movies x reviewers exclude real reviews: {}'.format(len(megamat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = pd.concat([megamat.drop(['joincol','joinId','userId','movieId','name','year','rating_y',\n",
    "                                 # 'metascore',\n",
    "                                 'imdb_id','poster','genre',\n",
    "                                 'certificate','runtime','director_actor', \n",
    "                                 # 'story', \n",
    "                                 'imdbId','tmdbId',\n",
    "                                 'title','genres','genre1',\n",
    "                                 'label','label_c',\n",
    "                                 # 'timestamp','vote',\n",
    "                                 'director','director1','director2','actor1'\n",
    "                                 ], axis = 1),\n",
    "               pd.get_dummies(megamat[[\n",
    "                   #'certificate',\n",
    "                   #'genre1',\n",
    "                   'label_c','director1','actor1']], \n",
    "                              drop_first = True, dummy_na = True)],axis = 1)\n",
    "\n",
    "# fill metascore null values with mean\n",
    "X_final['metascore'].fillna(X_final['metascore'].mean(), inplace=True)\n",
    "X_final.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1_final\n",
    "X1_final = []\n",
    "sentences = list(X_final[\"story\"])\n",
    "for sen in sentences:\n",
    "    X1_final.append(preprocess_text(sen))\n",
    "    \n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X1_final)\n",
    "\n",
    "X1_final = tokenizer.texts_to_sequences(X1_final)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 50 # 200\n",
    "\n",
    "X1_final = pad_sequences(X1_final, padding='post', maxlen=maxlen)\n",
    "\n",
    "# data scale\n",
    "# scaler1 = MinMaxScaler()\n",
    "# scaler1 = StandardScaler()\n",
    "# X1_final = scaler1.fit_transform(X1_final)\n",
    "\n",
    "# embedding\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file_link = p_data_src+'/glove.6B/glove.6B.50d.txt' # glove file link\n",
    "\n",
    "with open(glove_file_link, 'r') as glove_file:\n",
    "    for line in glove_file:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary [word] = vector_dimensions\n",
    "\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X2_final\n",
    "X2_final = X_final.drop('story', axis=1).copy().values\n",
    "\n",
    "# data scale\n",
    "X2_final = ratings_scaler2.fit_transform(X2_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 4,\n",
       "       4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 3,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       3, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 3, 4, 4, 4, 4, 3, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 3, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 3, 4, 4, 4,\n",
       "       4, 4, 3, 4, 4, 4, 3, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2,\n",
       "       4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 3, 3, 4, 3, 4, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4,\n",
       "       3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2,\n",
       "       3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3,\n",
       "       3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3,\n",
       "       2, 3, 3, 2, 3, 3, 3, 2, 2, 2, 3, 3, 4, 3, 3, 3, 4, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3,\n",
       "       3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2,\n",
       "       3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 4, 4, 3, 3, 3, 3, 4, 3, 3, 2, 4, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def return_prediction(model,scaler1,scaler2,sample_json):\n",
    "# def return_prediction(model, sample_json):\n",
    "#     story = [[]]\n",
    "#     # story = scaler1_final.transform(story)\n",
    "#     reviewer = [[]]\n",
    "#     # reviewer = scaler2_final.transform(reviewer)\n",
    "#     rating_ind = model.predict_classes([story, reviewer])\n",
    "\n",
    "# rating_ind = ratings_model.predict([X1_final, X2_final])\n",
    "\n",
    "cutoff = 1000\n",
    "rating_ind = ratings_model.predict([X1_final[:cutoff], X2_final[:cutoff]])\n",
    "\n",
    "predictions = np.argmax(rating_ind,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
